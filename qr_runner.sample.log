Loading pickled vobac in 2.7546210289
copying index from /srv/local/work/sixilu2/sixilu2/github/queryreformulator/QueryReformulator/data/index/ to ./index
Folder ./index already exists! Doing nothing.
Loading Title-ID mapping...
Loading queries and docs 100.495731831
agent.initialize_model(), do nothing.
agent.reset(), do nothing.
Starting QueryReformulatorAgent for Environment 'Query Reformulator Env'
agent.reset(), do nothing.
agent.reset(), do nothing.
in agent.act(), states = 
[]
current_queries (before calling search.perform) =  [['atmospheric', 'circulation', 'of', 'exoplanets', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
q_i =  [[ 19988 247603 113658 369760     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2]]
q_m =  [[1.0000 1.0000 1.0000 1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000]]
n_iter =  0
qs =  ['atmospheric circulation of exoplanets']
AAA 0 ['atmospheric circulation of exoplanets'] {0: ['atmospheric circulation of exoplanets']}
q =  atmospheric circulation of exoplanets
current_queries (after calling search.perform) =  [['atmospheric', 'circulation', 'of', 'exoplanets', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', u'atmospheric', u'circulation', u'of', u'tidally', u'locked', u'exoplanets', u'a', u'suite', u'of', u'benchmark', u'tests', u'for', u'dynamical', u'solvers', u'the']]
agent.observe(), terminal = 
False
reward = 
[0.0279]
in agent.act(), states = 
[[u'atmospheric circulation of exoplanets', u'gliese 581g as a scaled up version of earth atmospheric circulation simulations', u'thermodynamics of atmospheric circulation on hot jupiters', u'the role of drag in the energetics of strongly forced exoplanet atmospheres', u'atmospheric circulation and composition of gj1214b', u'atmospheric circulation of tidally locked exoplanets a suite of benchmark tests for dynamical solvers', u'influence of solar flares and disturbances of the interplanetary medium on the atmospheric circulation', u'geologic approach to the long term history of atmospheric circulation', u'atmospheric loss of exoplanets resulting from stellar x ray and extreme ultraviolet heating', u'atmospheric and oceanic fluid dynamics fundamentals and large scale circulation', u'the general circulation of the atmosphere', u'atmospheric circulation during warm geologic periods is the equator to pole surface temperature gradient the controlling factor', u'atmospheric circulation patterns during glacial inception a possible candidate', u'cretaceous climate a comparison of atmospheric simulations with the geologic record', u'coronal mass ejection cme activity of low mass m stars as an important factor for the habitability of terrestrial exoplanets ii cme induced ion pick up of earth like exoplanets in close in habitable zones', u'atmospheric circulation systems', u'global atmospheric circulation statistics', u'jupiter s atmospheric circulation', u'decadal oscillations of the air ice ocean system in the northern hemisphere', u'on the star magnetosphere interaction of close in exoplanets', u'atmospheric tides and the 4 day circulation on venus', u'global atmospheric circulation statistics 1000 1 mb', u'approximate methods for atmospheric and oceanographic circulation problems', u'global atmospheric circulation statistics 1958 1973', u'atmospheric circulation of hot jupiters a review of current understanding', u'a proposal for the intercomparison of the dynamical cores of atmospheric general circulation models', u'uncertainties in carbon dioxide radiative forcing in atmospheric general circulation models', u'ultra low frequency variability in a simple atmospheric circulation model', u'solar activity earth s pressure field and atmospheric circulation', u'simulation of the martian atmospheric polar warming with the lmd general circulation model', u'the cenozoic ocean circulation based on ocean general circulation model results', u'fundamentals of atmospheric physics', u'ranges of atmospheric mass and composition of super earth exoplanets', u'atmospheric circulation of tidally locked exoplanets ii dual band radiative transfer and convective adjustment', u'atmospheric science an introductory survey', u'interpretation of cloud climate feedback as produced by 14 atmospheric general circulation models', u'sensitivity of an atmospheric general circulation model to prescribed sst changes feedback effects associated with the simulation of cloud optical properties', u'atmospheric carbon dioxide and early eocene climate a general circulation modeling sensitivity study', u'a sensitivity study of changes in earth s rotation rate with an atmospheric general circulation model', u'martian atmospheric data assimilation with a simplified general circulation model orbiter and lander networks']]
current_queries (before calling search.perform) =  [['a', 'theory', 'of', 'historical', 'discovery', 'the', 'construction', 'of', 'componential', 'models', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
q_i =  [[318311 253088 113658  95326  54879 340991  60190 113658 229757 302521     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2]]
q_m =  [[1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000]]
n_iter =  1
qs =  ['a theory of historical discovery the construction of componential models']
AAA 1 ['a theory of historical discovery the construction of componential models'] {0: ['atmospheric circulation of exoplanets'], 1: ['a theory of historical discovery the construction of componential models']}
q =  a theory of historical discovery the construction of componential models
current_queries (after calling search.perform) =  [['a', 'theory', 'of', 'historical', 'discovery', 'the', 'construction', 'of', 'componential', 'models', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
agent.observe(), terminal = 
False
reward = 
[0.2000]
in agent.act(), states = 
[[u'a theory of historical discovery the construction of componential models', u'the search for regularity four aspects of scientific discovery', u'beyond iq a triarchic theory of human intelligence', u'components of individual differences in human intelligence', u'chemical discovery as belief revision', u'semantic networks of english', u'weak interactions of leptons and quarks', u'earthquakes and geological discovery', u'testing string theory with cmb', u'the formation of econometrics a historical perspective', u'foundations of computational linguistics human computer communication in natural language', u'componential analysis and the study of meaning', u'a componential self organising neural network', u'sketch of a componential subtheory of human intelligence', u'the processes of scientific discovery the strategy of experimentation', u'yankee kinship terminology a problem in componential analysis1', u'theories of delinquency an examination of explanations of delinquent behavior', u'intelligence information processing and analogical reasoning the componential analysis of human abilities', u'the psychology of facial expression a componential approach to the meaning of facial expressions', u'on inflation in string theory', u'the componential framework and its role in reusability', u'lectures on string theory', u'lectures on f theory compactifications and model building', u'four dimensional superstring models', u'direct detection of dark matter', u'componential analysis kinship studies in cultural anthropology are producing a new tool for semantic analysis', u'componential analysis of general vocabulary the semantic structure of a set of verbs in english hindi and japanese', u'scientific discovery computational explorations of the creative process', u'generalization with componential attractors word and nonword reading in an attractor network', u'topics from the theory of numbers', u'discoveries and the emergence of new fields in science', u'gauge theory historical origins and some modern developments', u'inflation in string theory', u'curiosities at c 2', u'topological m theory from pure spinor formalism', u'cortico hippocampal interplay and the representation of contexts in the brain', u'scientific theory formation through analogical inference', u'reflecting telescope optics i basic design theory and its historical development', u'1 emotion and behavior theory current research in historical perspective', u'power prior distributions for regression models']]
current_queries (before calling search.perform) =  [['use', 'of', 'activity', 'classes', 'in', 'adaptive', 'transform', 'image', 'coding', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
q_i =  [[348755 113658 300719  76596  92996  48344 171569 151523 346614     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2]]
q_m =  [[1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000]]
n_iter =  2
qs =  ['use of activity classes in adaptive transform image coding']
AAA 2 ['use of activity classes in adaptive transform image coding'] {0: ['atmospheric circulation of exoplanets'], 1: ['a theory of historical discovery the construction of componential models'], 2: ['use of activity classes in adaptive transform image coding']}
q =  use of activity classes in adaptive transform image coding
current_queries (after calling search.perform) =  [['use', 'of', 'activity', 'classes', 'in', 'adaptive', 'transform', 'image', 'coding', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
agent.observe(), terminal = 
False
reward = 
[0.0000]
in agent.act(), states = 
[[u'use of activity classes in adaptive transform image coding', u'image sequence coding using adaptive vector quantisation in wavelet transform domain', u'an adaptive transform coding algorithm', u'still picture compression algorithms evaluated for international standardisation', u'some experiments in adaptive and predictive hadamard transform coding of pictures', u'image representation using nonorthogonal basis images with adaptive weight optimization', u'transform image coding', u'performance of block cosine image coding with adaptive quantization', u'survey of adaptive image coding techniques', u'hybrid dft dpcm interframe image quantization', u'singular value decomposition svd image coding', u'optimally adaptive transform coding', u'prioritized dct for compression and progressive transmission of images', u'adaptive coding method of x ray mammograms', u'adaptive coding of monochrome and color images', u'adaptive transform coding of video signals', u'intraframe and interframe adaptive transform coding', u'adaptive transform coding of speech signals', u'anisotropic nonstationary image estimation and its applications part ii predictive image coding', u'channel rate equalization techniques for adaptive transform coders', u'interframe cosine transform image coding', u'theoretical performance models for interframe transform and hybrid transform dpcm coders', u'an image transform coding scheme based on spatial domain considerations', u'spatial transform coding of color images', u'an adaptive strategy for hybrid image coding', u'image compression through wavelet transform coding', u'coding television signals at 320 and 64 kbit s', u'combined source channel coding in adaptive transform coding systems for images', u'a contour texture approach to picture coding', u'hadamard transform image coding', u'vector transform and image coding', u'image coding by autoregressive synthesis', u'slant transform image coding', u'wavelets in image communication', u'color image compression by adaptive vector quantization', u'a tutorial on modern lossy wavelet image compression foundations of jpeg 2000', u'recursive block coding a new approach to transform coding', u'adaptive transform coding of color images at low rates', u'comparison of nth order dpcm encoder with linear transformations and block quantization techniques', u'image coding using vector quantization in the transform domain']]
current_queries (before calling search.perform) =  [['planning', 'and', 'problem', 'solving', 'from', 'neuropsychology', 'to', 'functional', 'neuroimaging', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
q_i =  [[323436 253156 188749 373703 102879 275199 256448  77522 117802     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2]]
q_m =  [[1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000]]
n_iter =  3
qs =  ['planning and problem solving from neuropsychology to functional neuroimaging']
AAA 3 ['planning and problem solving from neuropsychology to functional neuroimaging'] {0: ['atmospheric circulation of exoplanets'], 1: ['a theory of historical discovery the construction of componential models'], 2: ['use of activity classes in adaptive transform image coding'], 3: ['planning and problem solving from neuropsychology to functional neuroimaging']}
q =  planning and problem solving from neuropsychology to functional neuroimaging
current_queries (after calling search.perform) =  [['planning', 'and', 'problem', 'solving', 'from', 'neuropsychology', 'to', 'functional', 'neuroimaging', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
agent.observe(), terminal = 
False
reward = 
[0.0303]
in agent.act(), states = 
[[u'planning and problem solving from neuropsychology to functional neuroimaging', u'theory of constructions and set in problem solving', u'the functional neuroanatomy of the human orbitofrontal cortex evidence from neuroimaging and neuropsychology', u'8 the functional neuroimaging of memory disorders', u'the prefrontal cortex anatomy physiology and neuropsychology of the frontal lobe', u'the cognitive neuroscience of memory distortion', u'planning in a hierarchy of abstraction spaces', u'functional strips a more flexible language for planning and problem solving', u'cognitive planning in humans neuropsychological neuroanatomical and neuropharmacological perspectives', u'a multiagent planning architecture', u'qualitative and quantitative knowledge in classical mechanics', u'generating abstraction hierarchies an automated approach to reducing search in planning', u'coordination of distributed problem solvers', u'neuropsychology and the cognitive nature of the emotions', u'failsafe a floor planner that uses ebg to learn from its failures', u'planning and temporal reasoning under uncertainty', u'automatically generating abstractions for planning', u'integrating marker passing and problem solving a spreading activation approach to improved choice in planning', u'cognitive neuropsychology a clinical introduction', u'time saving tips for problem solving with incomplete information', u'span integrating problem solving tactics', u'a unified approach to dynamic coordination planning actions and interactions in a distributed problem solving network', u'a study of the performance of patients with frontal lobe lesions in a financial planning task', u'incremental planning to control time constrained blackboard based problem solver vehicle monitoring', u'neural modeling and functional neuroimaging', u'an integrative approach to high performance biomedical problem solving environments on the grid', u'a graphical programming language interface for an intelligent lisp tutor', u'control knowledge to improve plan quality', u'auditory hallucinations phenomenology neuropsychology and neuroimaging update', u'the neuropsychology of emotion', u'the neuropsychology of ventral prefrontal cortex decision making and reversal learning', u'meta planning representing and using knowledge about planning in problem solving and natural language understanding', u'planning and learning in permutation groups', u'learning by analogy formulating and generalizing plans from past experience', u'solving time dependent planning problems', u'gpt meets psr', u'classification problem solving', u'cognitive processes and ill defined problems a case study from design', u'verbal behavior and problem solving some effects of labeling in a functional fixedness problem', u'hierarchical planning in a distributed environment']]
current_queries (before calling search.perform) =  [['scaled', 'subprofile', 'model', 'a', 'statistical', 'approach', 'to', 'the', 'analysis', 'of', 'functional', 'patterns', 'in', 'positron', 'emission', 'tomographic', 'data', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
q_i =  [[ 45340     -1 160339 318311 258873  78035 256448 340991 277485 113658  77522 368214  92996 371918  36078 221431 104442     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2]]
q_m =  [[1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000]]
n_iter =  4
qs =  ['scaled subprofile model a statistical approach to the analysis of functional patterns in positron emission tomographic data']
AAA 4 ['scaled subprofile model a statistical approach to the analysis of functional patterns in positron emission tomographic data'] {0: ['atmospheric circulation of exoplanets'], 1: ['a theory of historical discovery the construction of componential models'], 2: ['use of activity classes in adaptive transform image coding'], 3: ['planning and problem solving from neuropsychology to functional neuroimaging'], 4: ['scaled subprofile model a statistical approach to the analysis of functional patterns in positron emission tomographic data']}
q =  scaled subprofile model a statistical approach to the analysis of functional patterns in positron emission tomographic data
current_queries (after calling search.perform) =  [['scaled', 'subprofile', 'model', 'a', 'statistical', 'approach', 'to', 'the', 'analysis', 'of', 'functional', 'patterns', 'in', 'positron', 'emission', 'tomographic', 'data', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
agent.observe(), terminal = 
False
reward = 
[0.0588]
in agent.act(), states = 
[[u'scaled subprofile model a statistical approach to the analysis of functional patterns in positron emission tomographic data', u'the metabolic topography of normal aging', u'functional neuroimaging technical foundations', u'data analysis paradigms for metabolic flow data combining neural modeling and functional neuroimaging', u'functional mapping of the human cerebellum with positron emission tomography', u'spatiotemporal analysis of event related fmri data using partial least squares', u'basal ganglia functional connectivity based on a meta analysis of 126 positron emission tomography and functional magnetic resonance imaging publications', u'functional neuroimaging studies of human emotions', u'confounded correlations statistical limitations in the analysis of interregional relationships of cerebral metabolic activity', u'motor learning in man a positron emission tomographic study', u'a noninvasive approach to quantitative functional brain mapping with h215o and positron emission tomography', u'the relationship between global and local changes in pet scans', u'positron emission tomographic studies of sensory stimuli cognitive processes and anxiety', u'the role of cerebral cortex in the generation of voluntary saccades a positron emission tomographic study', u'positron emission tomographic studies of regional cerebral glucose metabolism in idiopathic dystonia', u'the metabolic topography of parkinsonism', u'neuroanatomical correlates of a lactate induced anxiety attack', u'visual memory visual imagery and visual recognition of large field patterns by the human brain functional anatomy by positron emission tomography', u'detection of thirty second cognitive activations in single subjects with positron emission tomography a new low dose h215o regional cerebral blood flow three dimensional imaging technique', u'enhanced detection of focal brain responses using intersubject averaging and change distribution analysis of subtracted pet images', u'elucidating dynamic brain interactions with across subjects correlational analyses of positron emission tomographic data the functional connectivity of the amygdala and orbitofrontal cortex during olfactory tasks', u'a highly accurate method of localizing regions of neuronal activation in the human brain with positron emission tomography', u'functional neuroimaging of attention in the auditory modality', u'combination of dynamic and integral methods for generating reproducible functional cbf images', u'cerebral metabolism and atrophy in huntington s disease determined by 18fdg and computed tomographic scan', u'regional cerebral blood flow measured during symptom provocation in obsessive compulsive disorder using oxygen 15 labeled carbon dioxide and positron emission tomography', u'cerebral glucose metabolism in childhood onset obsessive compulsive disorder revisualization during pharmacotherapy', u'accuracy and precision of the computerized brain atlas programme for localization and quantification in positron emission tomography', u'a statistical model for positron emission tomography', u'localisation in pet images direct fitting of the intercommissural ac pc line', u'retinotopic organization of human visual cortex mapped with positron emission tomography', u'design considerations for a positron emission transaxial tomograph pett iii', u'positron emission tomographic measurements of pulvinar activity during an attention task', u'regional brain glucose metabolism in chronic schizophrenia a positron emission transaxial tomographic study', u'dynamic positron emission tomography for study of cerebral hemodynamics in a cross section of the head using positron emitting 68ga edta and 77kr', u'statistical parametric maps in functional imaging a general linear approach', u'clustered pixels analysis for functional mri activation studies of the human brain', u'the neural processing of complex sounds', u'functional connectivity the principal component analysis of large pet data sets', u'an approach to multivariate phase synchronization analysis and its application to event related potentials']]
current_queries (before calling search.perform) =  [['tensors', 'from', 'k3', 'orientifolds', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
q_i =  [[346056 102879     -1     -1     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2
      -2     -2     -2     -2     -2     -2     -2     -2     -2     -2     -2]]
q_m =  [[1.0000 1.0000 1.0000 1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000]]
n_iter =  5
qs =  ['tensors from k3 orientifolds']
AAA 5 ['tensors from k3 orientifolds'] {0: ['atmospheric circulation of exoplanets'], 1: ['a theory of historical discovery the construction of componential models'], 2: ['use of activity classes in adaptive transform image coding'], 3: ['planning and problem solving from neuropsychology to functional neuroimaging'], 4: ['scaled subprofile model a statistical approach to the analysis of functional patterns in positron emission tomographic data'], 5: ['tensors from k3 orientifolds']}
q =  tensors from k3 orientifolds
current_queries (after calling search.perform) =  [['tensors', 'from', 'k3', 'orientifolds', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]
agent.observe(), terminal = 
False
